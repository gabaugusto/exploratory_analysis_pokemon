# -*- coding: utf-8 -*-
"""Pokedex_DataAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/gabaugusto/exploratory_analysis_pokemon/blob/master/Pokedex_DataAnalysis.ipynb

# Introduction to Pokémon Data Analysis

Welcome to the exciting world of Pokémon and data analysis! For decades, Pokémon has been more than just a game or a franchise—it’s been a cultural phenomenon that bridges generations. With its rich universe of creatures, each with unique attributes, evolutions, and abilities, Pokémon offers an engaging and structured dataset that is perfect for anyone starting their journey in data science and machine learning.

Imagine delving into the stats of Pikachu, analyzing how Charizard’s attributes evolve, or predicting whether a mysterious new Pokémon belongs to the legendary class. The possibilities are as vast as the Pokédex itself! Working with Pokémon data not only makes learning fun and relatable but also equips you with foundational skills for tackling real-world datasets.

## Why Work with Pokémon Data?

The Pokémon dataset is a treasure trove for beginners because it strikes a perfect balance between complexity and accessibility. Here are a few reasons why it’s an ideal starting point:

**Diverse Features**: From heights and weights to battle stats like attack and speed, the data provides a mix of numeric, categorical, and textual attributes, offering ample opportunities to explore and apply different analytical techniques.

**Familiarity and Fun**: Who doesn’t love Pokémon? Using a dataset tied to a beloved franchise makes the learning process engaging and relatable, helping you stay motivated.

**Real-World Relevance**: Pokémon data mirrors the types of datasets you’ll encounter in professional settings, with imperfect entries, missing values, and the need for feature engineering. Tackling these challenges here prepares you for future projects.

**Broad Applications**: Whether it’s classification, clustering, or even predictive modeling, Pokémon data can serve as a sandbox for exploring various machine learning techniques.

## The Dataset We’re Using

Our dataset spans hundreds of Pokémon and their unique characteristics:

**Attributes**: Basic information such as name, type, height, weight, and generation.

**Battle Stats**: Metrics like HP (health points), attack, defense, and speed.

**Classifications**: Categories such as “ordinary,” “legendary,” and “baby” Pokémon.

**Forms and Variants**: Regional variants and special forms like Gigantamax.

**Relationships**: Evolutionary lines that show how Pokémon transform as they grow.

**IMPORTANT**: While the data is rich and varied, it’s not perfect. Some entries have missing values or inconsistencies, but that’s part of the learning experience. Together, we’ll clean, enrich, and refine this dataset, uncovering valuable insights along the way.

### Setting the Stage

As we embark on this journey, remember that working with Pokémon data isn’t just about crunching numbers—it’s about storytelling. Each analysis reveals something fascinating about these creatures and the universe they inhabit. By the end, you’ll not only have honed your technical skills but also gained a new appreciation for the power of data in uncovering hidden patterns and making informed predictions.

So, grab your Pokédex, and let’s dive into the world of Pokémon data analysis! Whether you’re a data science rookie or a seasoned enthusiast, this adventure promises to be both educational and thrilling.

**Let’s catch ‘em all—insights, that is!**

# 1 Understanding and Preprocess the Dataset

Dataset is clean and well-structured.

Handling missing values (e.g., fill, drop, or flag missing data).

Encode categorical data (e.g., types, abilities, and classifications) into numerical formats using one-hot encoding or label encoding.
"""

# Install the main libraries for this tutorial
# %pip install pandas seaborn matplotlib scikit-learn

# Short description of the libraries
# pandas: data manipulation and analysis
# seaborn: data visualization
# matplotlib: data visualization
# scikit-learn: machine learning

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("./PokemonData/pokemons_9gen.csv", sep = ";")

df.head()

"""# 2. Improving the Dataset

Most of the Pokémon data is already clean and well-structured, but there are still opportunities to enhance it further. Here are some ways to improve the dataset:

Feature Engineering: Create new features (e.g., total stats, stat ratios), dumming the types and the rank.

By "dumming" we are saying that we will create a new column for each type of Pokémon, and if the Pokémon has that type, we will put 1 in the column, otherwise, we will put 0. This is a way to encode the categorical data into numerical format.

### Define Your Features and Target

Features are the attributes that you will use to predict the target. In this case, the features are the Pokémon attributes (e.g., stats, types, abilities). It is important to select relevant features that can help predict the target accurately.

Target is the classification column (e.g., ordinary, legendary, baby). Is what you want to predict.

Features: Select relevant attributes such as stats (HP, Attack, Defense, etc.), types, abilities, height, weight, etc.

Target: The classification column (e.g., ordinary, legendary, baby).
"""

# Putting the dataset in the rightway:

# 1. Filling the blank spaces (if are any)
# fill na with none
df["classification"] = df["classification"].fillna("none")
df["regional_variant"] = df["regional_variant"].fillna("none")
df["form_variant"] = df["form_variant"].fillna("none")
df["gmax"] = df["gmax"].fillna("false")
df["type2"] = df["type2"].fillna("none")

# 2. Turn the values in to lowercase
df["classification"] = df["classification"].str.lower()
df["gmax"] = df["gmax"].str.lower()

# 3. dumming some columns. Turning quality values into booleans values so they can behave like numbers.

# Use pd.get_dummies instead of Series.get_dummies
df = pd.get_dummies(df, columns=["regional_variant", "form_variant", "classification", "rank", "generation", "gmax", "stage", "type1", "type2"])

df.head()

# Create a column named total_stats after speed column and sum the stas of "hp", "atk", "def",  "spatk",  "spdef",  "speed"

# Calculate total stats
df['total_stats'] = df['hp'] + df['atk'] + df['def'] + df['spatk'] + df['spdef'] + df['speed']

# Reorder columns to place 'total_stats' after 'speed'
cols = list(df.columns)
cols.insert(cols.index('speed') + 1, cols.pop(cols.index('total_stats')))
df = df[cols]

df.describe()

# Dealing with the abilities column

# Step 1: Split the abilities into separate words
df['abilities_split'] = df['abilities'].str.split()

# Step 2: Expand the lists into rows for easier processing
expanded_df = df.explode('abilities_split')

# Step 3: Create dummy columns using get_dummies
dummy_columns = pd.get_dummies(expanded_df['abilities_split'], prefix='abilities')

# Step 4: Aggregate dummy columns back to the original DataFrame structure
# Use the index of the concatenated DataFrame (or expanded_df.index) for grouping
result = pd.concat([df, dummy_columns], axis=1).groupby(dummy_columns.index).max()
# OR
# result = pd.concat([df, dummy_columns], axis=1).groupby(pd.concat([df, dummy_columns], axis=1).index).max()

# Drop the 'abilities_split' column (optional)
result = result.drop(columns=['abilities_split'])

df = df.drop(columns=['abilities_split'])

df = result

# removing the temporary datasets
del dummy_columns
del expanded_df
del result

df.head()

# count and sum and list every column that has na or null values

# Count and sum of NA/null values per column
na_counts = df.isna().sum()
na_columns = na_counts[na_counts > 0]

print("Columns with NA/null values:")
print(na_columns)

print("\nSum of NA/null values per column:")
na_counts

# Count and sum of NA/null values per column
na_counts = df.isna().sum()

# Filter columns with NA/null values
na_columns = na_counts[na_counts > 0]

# Print column names with NA/null values
print("Columns with NA/null values:")
print(na_columns)

# Print rows with NA/null values for each column
for col in na_columns.index:
  print(f"\nRows with NA/null values in column '{col}':")
  print(df[df[col].isna()])

"""Now we know that we have a dataset cleaned, the data is clean and we have 13 columns. Let's take a look at the first five rows of the dataset to understand the structure of the data.

# 3. Exploratory Data Analysis (EDA)

Here we navigate through the dataset to understand the distribution of features, identify patterns, and spot anomalies. This step is crucial for gaining insights and informing our subsequent analysis.

What we can do in this step:

- Visualize correlations between features and the target (e.g., Does legendary Pokémon might have higher stats?).

- Identify imbalanced classes (e.g., if most Pokémon are "ordinary" and very few are "legendary").

- What are the percentages of ranks of every pokemon type?

Lets start describing the Data to get some insights.
"""

df.describe()

# Calculate total records (excluding duplicates based on 'id')
total_records = df['id'].nunique()
print(f"Total records (unique IDs): {total_records}")

# Calculate records with type1_fire or type2_fire as True
type1_fire_count = df['type1_fire'].sum()
type2_fire_count = df['type2_fire'].sum()
fire_type_records = type1_fire_count + type2_fire_count - (df['type1_fire'] & df['type2_fire']).sum()

# Calculate and print the percentage
percentage_fire_type = (fire_type_records / total_records) * 100
print(f"Number of records with type1_fire or type2_fire: {fire_type_records}")
print(f"Percentage of records with type1_fire or type2_fire: {percentage_fire_type:.2f}%")

# Creating the bar chart

# Create lists to store type names and counts for the chart
type_names = []
type_counts = []

types = ["normal", "fighting", "flying", "poison", "ground", "rock", "bug", "ghost", "steel", "fire", "water", "grass", "electric", "psychic", "ice",	"dragon", "dark", "fairy"]

# Loop through each type and calculate/store the statistics
for type_name in types:
    type1_count = df[f'type1_{type_name}'].sum()
    type2_count = df[f'type2_{type_name}'].sum()

    # Calculate records with either type1 or type2 as True (avoiding double-counting)
    type_records = type1_count + type2_count - (df[f'type1_{type_name}'] & df[f'type2_{type_name}']).sum()

    type_names.append(type_name)
    type_counts.append(type_records)

# Create the bar chart
plt.figure(figsize=(12, 6))  # Adjust figure size as needed
plt.bar(type_names, type_counts)
plt.xlabel("Pokemon Type")
plt.ylabel("Number of Pokemon")
plt.title("Pokemon Type Distribution")
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

"""The percentages here do not help us much as there are lots of repeated IDs, alternative forms and so on...

Lets do the same to ranks.

In this sample we will use the term "competitive", that is the pokemon is on final stage and can be used in competitive battles.

This concept is open to debate, but we will consider the final stage of the pokemon evolution as competitive.

There too many pokemons and the ones that are not in final stage we don't consider as competitive.
"""

# Calculate total records (excluding duplicates based on 'id') (same as above)
total_records = df['id'].nunique()
print(f"Total pokemon (unique Pokedex IDs): {total_records}")

# To this sample we will use the term competitive.
df['rank_competitive'] = (df['stage_final'] == True) & (df['rank_ordinary'] == True)

# Define a list of ranks to iterate through
ranks = ['competitive', 'legendary', 'mega-evolution', 'paradox', 'baby']

# Loop through each rank and calculate/print the statistics
for rank_name in ranks:
    rank_count = df[f'rank_{rank_name}'].sum()

    percentage_rank = (rank_count / total_records) * 100

    print(f"Number of pokemon of rank {rank_name}: {rank_count}")
    print(f"Percentage of pokemons with rank {rank_name}: {percentage_rank:.2f}%\n")


# Creating the bar chart

# Create lists to store rank names and counts for the chart
rank_names = []
rank_counts = []

# Loop through each rank and calculate/store the statistics
for rank_name in ranks:
    rank_count = df[f'rank_{rank_name}'].sum()

    rank_names.append(rank_name)
    rank_counts.append(rank_count)

# Create the bar chart
plt.figure(figsize=(12, 6))  # Adjust figure size as needed
plt.bar(rank_names, rank_counts)
plt.xlabel("Pokemon Type")
plt.ylabel("Number of Pokemon")
plt.title("Pokemon Type Distribution")
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

# We can see a list of all the columns by name if needed.

# list(df.columns.values)

"""## Some corelations.

Here we will do the process two times: one using a single column as a Target and understaning the if, in total, Legendary Pokémon have better stats than Non-Legendary Pokémon. The other process we use multiple columns in the comparison.
"""

# Working with just one column as target

# Select relevant attributes and the target variable
features = ["height", "weight", "hp", "atk", "def", "spatk", "spdef", "speed", "total_stats"]
target = 'rank_legendary'

# Create a new DataFrame with selected features and target
df_selected = df[features + [target]]

# Display the first few rows of the new DataFrame
df_selected.head()

df.head()

# We are using 'df_selected' created in the previous code

# Separate legendary and non-legendary Pokémon
legendary_pokemon = df_selected[df_selected['rank_legendary'] == 1]
non_legendary_pokemon = df_selected[df_selected['rank_legendary'] == 0]

# Create box plots for each stat
features_to_plot = ["hp", "atk", "def", "spatk", "spdef", "speed", "total_stats"]

plt.figure(figsize=(15, 10))

for i, feature in enumerate(features_to_plot):
    plt.subplot(3, 3, i + 1)  # Adjust grid layout as needed
    sns.boxplot(x='rank_legendary', y=feature, data=df_selected)
    plt.title(f"Box Plot of {feature} for Different Ranks")
    plt.xlabel("Rank (0: Non-Legendary, 1: Legendary)")
    plt.ylabel(feature)

plt.tight_layout()  # Adjust spacing between plots
plt.show()

# Working with multiples columns as target

# Select relevant attributes and the target variable
multi_features = ["height", "weight", "hp", "atk", "def", "spatk", "spdef", "speed", "total_stats"]
multi_target = ['rank_legendary', 'rank_mega-evolution', "rank_baby"]

# Create a new DataFrame with selected features and target
# Use extend instead of + for combining the lists to avoid nesting
multi_features.extend(multi_target)  # This adds elements of target to the features list
# Include features in df_multi_selected
df_multi_selected = df[multi_features]

# Display the first few rows of the new DataFrame
df_multi_selected.head(10)

# We are using 'df_multi_selected' DataFrame created previously

# Map combined rank values to labels
rank_mapping = {
    0: 'Ordinary',
    1: 'Legendary',
    2: 'Mega-Evolution',
    3: 'Baby'
}

# Create the 'rank_combined' column based on the values of the other rank columns
df_multi_selected['rank_combined'] = df_multi_selected['rank_legendary'] + df_multi_selected['rank_mega-evolution'] * 2 + df_multi_selected['rank_baby'] * 3

df_multi_selected['rank_label'] = df_multi_selected['rank_combined'].map(rank_mapping)

# Create box plots for each stat
features_to_plot = ["hp", "atk", "def", "spatk", "spdef", "speed", "total_stats"]

plt.figure(figsize=(15, 10))

for i, feature in enumerate(features_to_plot):
    plt.subplot(3, 3, i + 1)
    sns.boxplot(x='rank_label', y=feature, data=df_multi_selected, order=list(rank_mapping.values()))
    plt.title(f"Box Plot of <{feature}> for Different Ranks")
    plt.xlabel("Rank")
    plt.ylabel(feature)
    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability

plt.tight_layout()
plt.show()

"""### Types and Stats relationship."""

# Select relevant attributes and target variables (types)
multi_type_features = ["height", "weight", "hp", "atk", "def", "spatk", "spdef", "speed", "total_stats"]
type_columns = [col for col in df.columns if col.startswith('type1_') or col.startswith('type2_')]

# Create a new DataFrame with selected features and target
df_multi_types_selected = df[multi_type_features + type_columns]

# Function to get Pokemon type
def get_pokemon_type(row):
    for type_col in type_columns:
        if row[type_col] == 1:
            # Extract type name from column name (e.g., 'type1_fire' -> 'fire')
            return type_col.split('_')[1]
    return None  # Handle cases where no type is found

# Apply the function to create a 'pokemon_type' column
df_multi_types_selected['pokemon_type'] = df_multi_types_selected.apply(get_pokemon_type, axis=1)

# Create box plots for each stat
features_to_plot = ["hp", "atk", "def", "spatk", "spdef", "speed", "total_stats"]
plt.figure(figsize=(40, 30))

for i, feature in enumerate(features_to_plot):
    plt.subplot(3, 3, i + 1)
    sns.boxplot(x='pokemon_type', y=feature, data=df_multi_types_selected)
    plt.title(f"Box Plot of {feature} for Different Types")
    plt.xlabel("Type")
    plt.ylabel(feature)
    plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.show()

"""### Identifing imbalanced classes

Identifying imbalanced classes is a critical step in understanding whether your dataset has a skewed distribution of target labels.

Imbalanced classes occur when the distribution of target classes in a dataset is not uniform, meaning that some classes have significantly more samples than others. For example, in this Pokémon dataset, there might be far more Pokémon classified as "ordinary" than as "legendary" or "baby."

The code below check if the legendary rank is unbalaced. There are many ways we could do this, but we can start this way.
"""

# Count the occurrences of each class in the target variable
class_counts = df_selected['rank_legendary'].value_counts()

# Print the class counts
print(class_counts)

# Calculate the percentage of each class
class_percentages = (class_counts / len(df_selected)) * 100

# Print the class percentages
print(class_percentages)

# Determine if there's class imbalance based on a threshold (e.g., 80/20 split)
imbalance_threshold = 80  # Example threshold. Diferent project will use different thresholds

if any(percentage >= imbalance_threshold for percentage in class_percentages):
    print("Class imbalance detected.")
else:
    print("No significant class imbalance detected.")

# Visualizations make it easier to detect imbalances.

# Bar Plot Example:
y = df_selected['rank_legendary']
sns.countplot(x=y)
plt.title("Class Distribution")
plt.show()

"""As we can see there are imbalanced classes.

### Attention to

**Severe Imbalance:** If one class constitutes over 75%-80% of the data, it indicates a severe imbalance.

**Moderate Imbalance:** If the largest class has 60%-75% representation, the imbalance is moderate.

**Balanced Data:** Ideally, all classes should have roughly equal representation (close to 33% for three classes).

#### Implications of Imbalanced Classes

Training Challenges: Models tend to predict the majority class more often because it minimizes errors. Minority classes may be ignored.

Evaluation Issues: Accuracy becomes unreliable as a performance metric. Precision, recall, and F1-score become more relevant.

#### Apply Resampling Techniques:

- Oversample the minority classes.

- Undersample the majority classes.

- Use synthetic data generation techniques (e.g., SMOTE).

**Use Class-Weighted Models:**

Adjust the algorithm to account for class imbalances (e.g., using class_weight='balanced').

**Choose Better Metrics:**

Use metrics like precision, recall, F1-score, or area under the ROC curve (AUC-ROC) instead of plain accuracy.

We will use SMOTE technique in a few moments.

## 4. Split the Dataset

### What we are doing

We are dividing your dataset into two subsets:

**Training Set:** Used to train your machine learning model. The model learns patterns from this subset.

**Testing Set:** Used to evaluate how well the model generalizes to unseen data.

This split ensures that the model is not tested on the same data it was trained on, which helps avoid overfitting (when the model performs well on the training data but poorly on new data).

### What to expect

After this step, we will have four subsets:

* **x_train**: Features for training (80% of the data by default).

* **y_train**: Target values for training.

* **x_test**: Features for testing (20% of the data).

* **y_test**: Target values for testing.
"""

# %pip install scikit-learn

from sklearn.model_selection import train_test_split

# Assuming 'df_selected' is your DataFrame with features and the target variable 'rank_legendary'
# Or use 'df_multi_selected' if you are working with multiple target variables
x = df_selected.drop('rank_legendary', axis=1)  # Features
y = df_selected['rank_legendary']  # Target variable

# randomize rows in the dataset to prevent bias
df_selected = df_selected.sample(frac=1, random_state=42)

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42) # 80% training and 20% test

# Print the shapes of the resulting sets to verify the split
print("x_train shape:", x_train.shape)
print("x_test shape:", x_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""## Preventing BIAS using SMOTE

The code above is completely fine as it is, but there are some situations where we could try to do things diferentely. As we saw above there are imbalanced classes in this dataset.

So we are going to use a slight differente code to train our data.


"""

## imbalanced-learn
# %pip install imbalanced-learn

from imblearn.over_sampling import SMOTE # pip install imbalanced-learn
from sklearn.model_selection import train_test_split

# Assuming you have a DataFrame 'df' with features and target
x = df_selected.drop('rank_legendary', axis=1)  # Features
y = df_selected['rank_legendary']  # Target variable

# Train-test split (stratify ensures class balance in test set)
x_train_smote, x_test_smote, y_train_smote, y_test_smote = train_test_split(
    x, y, test_size=0.2, stratify=y, random_state=42
)

## Apply SMOTE to the Training Data
## SMOTE generates synthetic examples for the minority classes in the training set.

smote = SMOTE(random_state=42)

# Fit and resample the training data
x_train_resampled, y_train_resampled = smote.fit_resample(x_train_smote, y_train_smote)

# Print the new class distribution
from collections import Counter
print("Before SMOTE:", Counter(y_train_smote))
print("After SMOTE:", Counter(y_train_resampled))

"""## 5. Choose a Model

### What You’re Doing
You’re selecting a machine learning algorithm to build your classifier. This step involves:

* Picking a model that suits your problem.

* Training the model on the training set (x_train, y_train).

* Using the trained model to make predictions.

Several classification algorithms can be used, such as:

- Logistic Regression
- Decision Tree
- Random Forest
- Gradient Boosting (e.g., XGBoost, LightGBM)
- Support Vector Machines (SVM)

### Why It’s Important

Different algorithms (e.g., Logistic Regression, Random Forest, etc.) have different strengths and weaknesses. For example:

**Random Forest** is good for handling non-linear data and doesn’t require much parameter tuning.

**Logistic Regression** is simpler and faster but may not capture complex relationships.

We will start with a simple model (e.g., Logistic Regression) to create a baseline.

### What to Expect
After training, the model will have learned patterns in the training data. You can now use it to make predictions on the testing data.
"""

## Below we are using the data WITHOUT SMOTE technique

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the Logistic Regression model
model = LogisticRegression(max_iter=1000) # Increased max_iter
model.fit(x_train, y_train)

# Make predictions on the test set
y_pred = model.predict(x_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

print(classification_report(y_test, y_pred))

# Plot confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Non-Legendary', 'Legendary'],
            yticklabels=['Non-Legendary', 'Legendary'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

## Below we are using the data USING SMOTE technique

# Initialize and train the Logistic Regression model
model = LogisticRegression(max_iter=1000) # Increased max_iter
model.fit(x_train_resampled, y_train_resampled)

# Make predictions on the test set
# Changed from: y_pred = model.predict(x_train_resampled)
y_pred = model.predict(x_test_smote) # Predict on the original test set (x_test)

# Evaluate the model
accuracy = accuracy_score(y_test_smote, y_pred) # y_test_smote is the correct target for the test set
print(f"Accuracy: {accuracy}")

print(classification_report(y_test_smote, y_pred))

# Plot confusion matrix
cm = confusion_matrix(y_test_smote, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Non-Legendary', 'Legendary'],
            yticklabels=['Non-Legendary', 'Legendary'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""## 6. Train the Model

### What You’re Doing

You’re using the training data (X_train and y_train) to "teach" your machine learning model to recognize patterns and relationships in the data. During this process, the model learns to associate input features (e.g., stats, types) with the target labels (e.g., classification as legendary, baby, ordinary).

**This step includes:**

Initializing the model.
Fitting the model to the training data.
Why It’s Important
This is the core learning process where the algorithm develops its understanding of the data. How well this is done determines how accurately the model will make predictions.

**Going deep**: A good model captures patterns in the data without overfitting.
An overfit model memorizes the training data and performs poorly on unseen data.
An underfit model fails to capture enough patterns and performs poorly on both training and testing data.
"""

from sklearn.ensemble import RandomForestClassifier

# Example using Random Forest
model = LogisticRegression(max_iter=100000) # Increased max_iter
model.fit(x_train, y_train)

## Using SMOTE
# Here we do the same as above but using the data with SMOTE technique

from sklearn.ensemble import RandomForestClassifier

# Example using Random Forest
modelSmote = LogisticRegression(max_iter=100000) # Increased max_iter  # Adjust values as needed
modelSmote.fit(x_train_resampled, y_train_resampled)

"""### What to Expect

After calling _model.fit()_, the model is now trained.

It has learned patterns from the training data.
You can use the trained model to make predictions on new or unseen data.

**Optional**: Evaluate Training Performance
If you want to see how well the model learned the training data, you can check its performance on x_train as a quick validation step. This is useful for detecting overfitting.


"""

from sklearn.metrics import accuracy_score

# Make predictions on the training set
y_train_pred = model.predict(x_train)

# Calculate accuracy on the training set
train_accuracy = accuracy_score(y_train, y_train_pred)
print(f"Training Accuracy: {train_accuracy:.2f}")

## Using SMOTE data

# Make predictions on the training set
y_train_pred = modelSmote.predict(x_train_resampled)

# Calculate accuracy on the training set
train_accuracy = accuracy_score(y_train_resampled, y_train_pred) # Use y_train_resampled instead of y_train_smote
print(f"Training Accuracy: {train_accuracy:.2f}")

"""The first Training Accuracy result (1.00) probably means that the model is overfitting the data. Overfitting occurs when the model learns the training data too well, including the noise and outliers, which can hurt its performance on new data.

The second Training Accuracy result (0.94) is more realistic and indicates that the model is learning well. The model is capturing patterns in the data without memorizing it.

After Training
Once the model is trained, you can proceed to:

* Predict the classifications for the testing set (x_test or x_test_resampled) using model.predict().

* Evaluate the predictions (as described in Step 7).

### How to Make Predictions

You can use the .predict() method of your trained model to predict the classifications of new data.

1. Predictions on the Test Set
This allows you to evaluate the model's performance by comparing its predictions (y_pred) to the actual labels (y_test).

### What to Expect

y_pred will be an array of predicted labels corresponding to the rows in X_test. For example:
"""

## Expected output
# Predictions: ['ordinary', 'ordinary', 'legendary', 'baby', 'ordinary', 'ordinary', ...]

# Shuffle the test set to avoid patterns
x_test = x_test.sample(frac=1, random_state=42)

# Use the trained model to make predictions on the test set
y_pred = model.predict(x_test)

# Print some sample predictions
print("Predictions:", y_pred[:150])

# Shuffle the test set to avoid patterns
x_test_resampled = x_test.sample(frac=1, random_state=42)

# Use the trained model to make predictions on the test set
y_pred = modelSmote.predict(x_test)

# Print some sample predictions
print("Predictions:", y_pred[:150])  # First 10 predictions

"""### 2. Predictions on New Data

You can also predict classifications for Pokémon that weren’t part of the training or testing sets.
"""

# Example of new Pokémon data (height, weight, hp, atk, def, spatk, spdef, speed, total_stats)

new_pokemon = [[1.0, 10.5, 45, 49, 49, 65, 65, 45, sum([45, 49, 49, 65, 65, 45])],  # Bulbasaur-like stats
                [2.0, 95.0, 100, 120, 90, 150, 140, 95, sum([100, 120, 90, 150, 140, 95])],
                [3.0, 120, 120, 120, 120, 120, 120, 120, sum([120, 120, 120, 120, 120, 120])]]  # Legendary-like stats

# Convert new Pokémon data to DataFrame with the same column names as the training data
new_pokemon_df = pd.DataFrame(new_pokemon, columns=x_train.columns)

# Make predictions for new data
new_predictions = model.predict(new_pokemon_df)
print("New Predictions:", new_predictions)

# Using SMOTE data/model
# Pokémon data (height, weight, hp, atk, def, spatk, spdef, speed, total_stats)

new_pokemon = [[1.0, 10.5, 45, 49, 49, 65, 65, 45, sum([45, 49, 49, 65, 65, 45])],  # Bulbasaur-like stats
                [2.0, 95.0, 100, 120, 90, 150, 140, 95, sum([100, 120, 90, 150, 140, 95])],
                [3.0, 120.0, 120, 120, 120, 120, 120, 120, sum([120, 120, 120, 120, 120, 120])]]  # Legendary-like stats

# Convert new Pokémon data to DataFrame with the same column names as the training data
new_pokemon_df = pd.DataFrame(new_pokemon, columns=x_train_resampled.columns)

# Make predictions for new data
new_predictions = modelSmote.predict(new_pokemon_df)
print("New Predictions:", new_predictions)

"""### Expected Output

For the above new_pokemon input, you might get predictions like:
"""

## New Predictions: ['ordinary', 'legendary']

"""## 7. Evaluating the Model

Check the model's performance using metrics like:

- Accuracy
- Precision, Recall, F1-Score (especially for imbalanced classes)
- Confusion Matrix
- ROC-AUC Score (if applicable)

### What we are doing?

You’re checking how well the model performs by comparing its predictions on the testing set (x_test) to the actual labels (y_test). Evaluation metrics tell you:

#### **How accurate is the model?**

Does the model perform equally well for all classes?

Are there any areas for improvement?

#### **Common Metrics**

**Accuracy:** Percentage of correct predictions.

**Precision and Recall:** Useful when dealing with imbalanced datasets.

**Precision:** How many predicted classifications were correct?

**Recall:** How many actual classifications were correctly identified?

**F1-Score:** Combines precision and recall into one metric.

**Confusion Matrix:** Shows how many predictions were correct and incorrect for each class.

### What to Expect

* **Classification Report:** Includes precision, recall, and F1-score for each class. Example:


* **Confusion Matrix:** A grid showing how many predictions were correct (diagonal) versus incorrect (off-diagonal). Example:
"""

from sklearn.metrics import classification_report, confusion_matrix

# Make predictions on the testing set
y_pred = model.predict(x_test)

# Print evaluation metrics
print("Classification Report:")
print(classification_report(y_test, y_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

"""## How to Interpret

High precision and recall scores for each class mean the model is performing well.

Look for errors in the confusion matrix to see where the model struggles (e.g., confusing "baby" with "ordinary").

Use these insights to improve the model (e.g., feature engineering, trying a different algorithm, or adjusting hyperparameters).

## Conclusion

Congratulations! You’ve successfully built and evaluated a Pokémon classification model. You’ve learned how to:

- Prepare and preprocess the dataset.

- Explore and visualize the data.

- Train and evaluate a machine learning model.

- Interpret the results and identify areas for improvement.

This project is just the beginning of your data science journey. You can further enhance the model by:

- Trying different algorithms (e.g., Random Forest, Gradient Boosting).

- Tuning hyperparameters to improve performance.

Thank you for joining us on this adventure through the Pokémon universe. We hope you’ve enjoyed exploring the data and building your first classification model. Remember, the world of data science is vast and full of exciting possibilities. Keep learning, experimenting, and discovering new insights. Who knows what amazing discoveries await you in your next project?
"""